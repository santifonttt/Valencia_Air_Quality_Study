---
title: "Estudio Contaminantes Conjunto"
output: html_document
date: "2025-05-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(naniar)
library(lubridate)
library(zoo)
library(stringr)
library(emmeans)
library(arrow)
library(forecast)
library(car)
library(lmtest)
library(emmeans)
library(mice)
library(FactoMineR)
library(factoextra)
library(pcaMethods)
```


```{r}
df <- read_parquet("../../../data/cleaned/calidad_aire/base_para_obj1.parquet")
df$FechaHora <- as.POSIXct(df$FechaHora, format = "%Y-%m-%d %H:%M:%S")
df$Origen <- as.factor(df$Origen)
```


# Selección de variables

Para este análsis vamos a utilizar las variables de interés que son las siguientes:

- FechaHora 
- Origen
- $NO$
- $NO_2$
- $NO_x$
- $O_3$
- $PM_{10}$
- $PM_{2.5}$
- $SO_2$

```{r}
df <- df %>% 
  select(FechaHora, Origen, NO, NO2, NOx, O3, PM10, PM2.5, SO2)
```


# Selección de estaciones

Para seleccionar las estaciones vamos a ver aquellas que tienen todos los contaminantes en común. 

Antes de ello vamos a ver el número de faltantes por contaminante y por estación.

```{r}
missing_pct <- df %>%
  pivot_longer(
    cols      = c(NO2, NO, NOx, PM10, `PM2.5`, SO2, O3),
    names_to  = "Contaminante",
    values_to = "Valor"
  ) %>%
  group_by(Origen, Contaminante) %>%
  summarise(
    Porcentaje_Faltantes = sum(is.na(Valor)) / n() * 100,
    .groups = "drop"
  ) %>%
  arrange(Porcentaje_Faltantes) %>%

print(missing_pct)
```

```{r}
miss_var_summary(df)
```

Vamos a eliminar aquellas estaciones que tengan un 100% de datos faltantes en algún contaminante.


```{r}
df <- df %>%
  group_by(Origen) %>%
  filter(if_all(
    c(NO2, NO, NOx, PM10, `PM2.5`, SO2, O3),
    ~ mean(is.na(.)) < 1
  )) %>%
  ungroup()
```


```{r}
miss_var_summary(df)
```

Ahora vamos a comprobar que no exista algún año con un 100% de faltantes para alguno de los contaminantes.

```{r}
missing_pct_year <- df %>%
  mutate(year = year(FechaHora)) %>%
  pivot_longer(
    cols      = c(NO2, NO, NOx, PM10, `PM2.5`, SO2, O3),
    names_to  = "Contaminante",
    values_to = "Valor"
  ) %>%
  group_by(year, Contaminante) %>%
  summarise(
    PctMissing = mean(is.na(Valor)) * 100,
    .groups    = "drop"
  )

print(missing_pct_year)
```

Hay muy poco porcentaje de faltantes en los contaminantes, por lo que no es necesario eliminar ningún año.

# Imputación de los faltantes

Vamos a probar a hacer una imputación de los faltantes con la librería mice. Hay que comprobar si la distribución de cada una de las variables cambia tras la imputación.

```{r}
df2 <- df %>%
  mutate(
    month = month(FechaHora),
    hour  = hour(FechaHora)
  ) %>%
  select(-FechaHora)

# 2) Inicializar mice para obtener estructura de métodos y predictorMatrix
init <- mice(df2, m = 1, maxit = 0, printFlag = FALSE)
methods    <- init$method
pred_matrix <- init$predictorMatrix

# 3) Definir las variables a imputar 
contaminantes <- c("NO2", "NO", "NOx", "PM10", "PM2.5", "SO2", "O3")

# 4) Asignar 'pmm' solo a los contaminantes, y '' al resto
methods[] <- ""
methods[contaminantes] <- "pmm"

# 5) Ejecutar la imputación
set.seed(123)
imp <- mice(
  data            = df2,
  method          = methods,
  predictorMatrix = pred_matrix,
  m               = 5,
  maxit           = 10,
  printFlag       = FALSE
)

# 6) Extraer la primera base completa imputada
df_imputado <- complete(imp, action = 1)
```

```{r}
all.equal(nrow(df), nrow(df_imputado))   # TRUE

# 2) Reincorpora FechaHora desde el df original
df_imputado <- df_imputado %>%
  mutate(FechaHora = df$FechaHora) %>%
  # opcional: mueve FechaHora al principio
  select(FechaHora, everything())
```


```{r}
# Comprobar la distribución de los contaminantes antes y después de la imputación
par(mfrow = c(2, 4))
for (contaminante in contaminantes) {
  # Histograma antes de la imputación
  hist(df2[[contaminante]], main = paste("Antes de la imputación:", contaminante), xlab = contaminante)
  
  # Histograma después de la imputación
  hist(df_imputado[[contaminante]], main = paste("Después de la imputación:", contaminante), xlab = contaminante)
}
```


```{r}
ggplot(df_imputado, aes(x = FechaHora)) +
  geom_line(aes(y = NOx, color = "NOx")) +
  geom_line(aes(y = NO2, color = "NO2")) +
  geom_line(aes(y = NO, color = "NO")) +
  labs(title = "Contaminantes NOx, NO2 y NO a lo largo del tiempo",
       x = "Fecha y Hora",
       y = "Concentración (µg/m³)",
       color = "Contaminante") +
  theme_minimal()
```

El comportameinto es prácticamente igual. Por lo tanto vamos a usar este dataset.

```{r}
write_parquet(df_imputado, "../../../data/cleaned/calidad_aire/base_para_obj1_imputado.parquet")
```



# Comportamiento entre contaminantes

Para ver patrones entre contaminantes vamos a realizar, en primer lugar, un PCA. Vamos a probar haciéndolo tanto con NIPALS para evitar problemas con NAs, como con vectores propios eliminando previamente los NAs.

```{r}
# Eliminar las variables month y hour
df_imputado <- df_imputado %>% 
  select(-month, -hour)
df_imputado_2 <- df_imputado %>% 
  select(-FechaHora)
```

```{r}
df_sample_2023 <- df_imputado %>%
  filter(year(FechaHora) == 2023) %>%
  sample_n(20000)

df_imputado_2 <- df_sample_2023 %>% 
  select(-FechaHora)

df_imputado_3 <- df_imputado_2 %>% 
  mutate(Origen_num = as.numeric(Origen)) %>% 
  select(-Origen)
```




```{r}
pca_contaminantes <- PCA(
  df_imputado_3,
  quali.sup   = 1,
  scale.unit  = TRUE,
  graph       = FALSE
)
```

```{r}
fviz_screeplot(pca_contaminantes, addlabels = TRUE, barfill = "steelblue")
```

```{r}
df_pca_20k <- df_imputado %>%
  filter(year(FechaHora) == 2023) %>%
  sample_n(20000) %>%
  select(Origen, NO, NO2, NOx, O3, PM10, `PM2.5`, SO2)

# 2) Ejecutar PCA (Origen como variable cuali. suplementaria en la col. 1)
res.pca <- PCA(
  df_pca_20k,
  quali.sup   = 1,
  scale.unit  = TRUE,
  graph       = FALSE
)

# 3) Scree‐plot
fviz_screeplot(res.pca,
               addlabels = TRUE,
               barfill   = "steelblue")

# 4) Biplot PC1 vs PC2
#    Sin repel y sin elipses para acelerar el render
fviz_pca_biplot(
  res.pca,
  habillage    = "Origen",
  addEllipses  = FALSE,
  repel        = FALSE,
  pointsize    = 1.5,
  label = "var"
)
```

Vamos a hacer ahora el PCA agrupando las estaciones por su localización geográfica.

- Puerto: València - Av. França, València Port llit antic Túria, València Port Moll Trans. Ponent

- Extrarradio: Quart de Poblet, Torrent-El Vedat

- València - Molí del Sol

- València - Pista de Silla

- València - Politècnic


```{r}
df_pca_grp <- df_imputado %>%
  filter(year(FechaHora) == 2023) %>%
  sample_n(20000) %>%
  mutate(
    Grupo = case_when(
      Origen %in% c(
        "València - Av. França",
        "València Port llit antic Túria",
        "València Port Moll Trans. Ponent"
      ) ~ "Puerto",
      Origen %in% c("Quart de Poblet", "Torrent-El Vedat") ~ "Extrarradio",
      Origen == c("València - Molí del Sol", "València - Pista de Silla", "València - Politècnic") ~ "Ciudad",
      TRUE                                          ~ NA_character_
    )
  ) %>%
  filter(!is.na(Grupo)) %>%
  select(Grupo, NO, NO2, NOx, O3, PM10, `PM2.5`, SO2)

# 2) PCA (Grupo como variable cuali.sup en la primera columna)
res.pca_grp <- PCA(
  df_pca_grp,
  quali.sup   = 1,
  scale.unit  = TRUE,
  graph       = FALSE
)

# 3) Scree‐plot
fviz_screeplot(res.pca_grp,
               addlabels = TRUE,
               barfill   = "steelblue")

# 4) Biplot PC1 vs PC2 coloreado por Grupo
fviz_pca_biplot(
  res.pca_grp,
  habillage    = "Grupo",
  addEllipses  = TRUE,
  ellipse.level = 0.95,
  repel        = TRUE,
  label        = "var"
)
```


Ahora vamos a hacer un clustering para ver si podemos agrupar las estaciones en función de los contaminantes. Vamos a usar el método de Ward y la distancia euclídea.

```{r clustering_estaciones, message=FALSE}

# 1) Agregar la concentración media de cada contaminante por estación
df_sta <- df_imputado %>%
  group_by(Origen) %>%
  summarise(across(
    c(NO, NO2, NOx, O3, PM10, `PM2.5`, SO2),
    ~ mean(.x, na.rm = TRUE)
  )) %>%
  column_to_rownames("Origen")

# 2) Escalar los datos
df_scaled <- scale(df_sta)

# 3) Elbow method: varianza intra‐cluster vs k
fviz_nbclust(df_scaled, kmeans, method = "wss") +
  labs(
    subtitle = "Elbow method: within‐cluster sum of squares"
  )

# 4) Silhouette method: coeficiente medio de silueta vs k
fviz_nbclust(df_scaled, kmeans, method = "silhouette") +
  labs(
    subtitle = "Silhouette method: average silhouette width"
  )
```
